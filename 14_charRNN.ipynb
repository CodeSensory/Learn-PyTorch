{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a28997af-4926-458c-9df0-c2a8e55c57c3",
   "metadata": {},
   "source": [
    "### Short Word - Character Level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4006718-3967-4e58-a834-75d973d231e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7652e4c-ccc4-4aca-8e45-b6269f6ddc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str + label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ac3d44-a78a-4de9-ac19-c04e3d8e1d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4852aed1-bf72-4195-bac3-c6adf03e63f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab))\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f00b3f-df04-45c0-861e-a47f8e54bcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30ea687b-7c06-4504-bbfc-d9258b5a7370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9184ccdb-6de2-4df3-956c-f4f1fec38295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f77604-c724-404c-9e74-57ca282edd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamco\\AppData\\Local\\Temp\\ipykernel_11248\\306303581.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2292800-d5a3-401f-828d-70e8a2eee171",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d003e1c8-d2a8-457e-a181-14943e05965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bac293d-3fef-4b5a-80d8-69522482bf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ecf60a4-4ff7-4a7f-b1f3-0df6ce49088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd013277-084e-40ae-9842-74ed52586982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.6452080011367798 prediction:  [[3 3 3 3 3]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  lllll\n",
      "1 loss:  1.4056836366653442 prediction:  [[4 4 4 0 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppp!e\n",
      "2 loss:  1.2222589254379272 prediction:  [[4 4 4 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppp!!\n",
      "3 loss:  1.0258066654205322 prediction:  [[4 4 4 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppp!!\n",
      "4 loss:  0.8298776745796204 prediction:  [[4 4 3 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplee\n",
      "5 loss:  0.6569910049438477 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.5194292068481445 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.40987300872802734 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.318220853805542 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.23626144230365753 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.16569429636001587 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.11361341178417206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.07951047271490097 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.05790004879236221 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.043980084359645844 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.03465989977121353 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.02811407670378685 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.023295732215046883 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.019609492272138596 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.01671212539076805 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.014396922662854195 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.012529509142041206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.011014756746590137 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.009780623950064182 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.008769788779318333 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.007936274632811546 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.007243156433105469 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.006660724524408579 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.006165837403386831 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0057404423132538795 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.005370518658310175 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.005045427940785885 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0047569903545081615 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.004499174654483795 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.0042672064155340195 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.004057580139487982 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0038673963863402605 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0036944888997823 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0035368981771171093 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0033932365477085114 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.003262016223743558 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0031422018073499203 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0030327090062201023 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.002932572038844228 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.002840893343091011 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0027569204103201628 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0026799929328262806 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0026093085762113333 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.002544229384511709 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0024842615239322186 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0024288371205329895 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0023774837609380484 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0023297297302633524 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.002285195980221033 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.002243551891297102 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.002204442862421274 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0021677035838365555 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0021328837610781193 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.002099983859807253 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.002068696077913046 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0020388313569128513 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0020103887654840946 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.001983060734346509 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0019568472635000944 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0019316778052598238 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0019073858857154846 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0018839715048670769 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0018613633001223207 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0018394667422398925 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0018182583153247833 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0017976423259824514 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0017777144676074386 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0017583802109584212 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.0017394956666976213 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0017211571102961898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0017033163458108902 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0016859503230080009 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0016689638141542673 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0016524277161806822 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0016362236347049475 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0016204232815653086 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0016049547120928764 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0015897946432232857 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0015750143211334944 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0015604946529492736 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.001546212239190936 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0015322382096201181 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0015184776857495308 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0015050022630020976 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.001491763861849904 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0014787389663979411 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.001465903827920556 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0014533060602843761 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0014408505521714687 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.001428608549758792 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.001416580518707633 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0014047420118004084 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0013930459972470999 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0013815161073580384 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0013701284769922495 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    result = outputs.data.numpy().argmax(axis=2)\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907517d-636d-4aa3-af0b-649970f07d8b",
   "metadata": {},
   "source": [
    "### Long Word - Character Level RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4269ff51-d806-47aa-8d14-d5e355e54c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb854143-786a-474f-af1e-59d9d9e9c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53b396cf-07b1-404f-a720-5f0e3b804243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0, 'o': 1, 'h': 2, 'i': 3, 'd': 4, 'w': 5, 's': 6, ',': 7, '.': 8, 'b': 9, 'u': 10, 't': 11, 'm': 12, 'a': 13, 'g': 14, 'r': 15, 'l': 16, 'k': 17, 'y': 18, 'p': 19, 'c': 20, 'e': 21, \"'\": 22, 'n': 23, ' ': 24}\n",
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {c: i for i, c in enumerate(char_set)}\n",
    "print(char_dic)\n",
    "\n",
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a90d0221-20f7-4624-a72b-7a39596a153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = dic_size\n",
    "sequence_length = 10  #random\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc02a0d3-1b31-4832-bb2f-0e433b9d5cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "x_data, y_data = [], []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str])\n",
    "    y_data.append([char_dic[c] for c in y_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bf82367-1650-4931-a8fb-dbf779a806ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n"
     ]
    }
   ],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a61cf7b-681d-4258-aa45-c26d441258ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layers):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a10842e-2f96-4218-9301-2c186875e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(dic_size, hidden_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb1e5f03-645b-44d0-b6f0-ee55023a1f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1005b39a-cecf-439d-8268-da1dd18ba3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68df590d-48d6-4764-bca3-bad8fdb6e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ob'i'''''''i''b'''''''''''i'''''''i'''pb'''''''''b''''''''b''b'''''''i'''''''''p''''i'''''''''''b'''''''''''''''''''i''''''b''''''''''b''b''r'b'''''''''''''''b''''''''i'''b'''b'''\n",
      "                                                                                                                                                                                   \n",
      "tosootsoosorootsooaotosooototoootooootsaoto.oslots.ooooos.toaoooooososoooososlsoootooosrrto.ooootstsrot.gos.oosoo.oo.otsaost.aoros.oo.roao.oottoootooo.rst.rs.r.oo.ros.ooootooosros\n",
      "eototttotettottttttttttetototoettotottttttttottottotttotottottoettttttottttteetotttotttttottottttotttetoettotttotttetotottttotttetottttottoetttoettototottttettotttettettoetotttott\n",
      " ldldlt tdtdldt  td  d td t t  hd     t tdtd t   t   d t     td tde  t     t td dht t   t t   edl  tdlt  l    t t     t t d   td t    l  td tdth tdld   dl  t t l tdtd  dolto hde  \n",
      "   hdhth h ih  hh h to hdh h   h h   hdh h hrghd thhhlhhh td ih kh h ih h th  ho,h h h  th t    ih  hihh   h h h h hth h hdt  r  hh h to  h k  h  th   hw    h h hihth  hohth h hw \n",
      "         h h  h                 th      h                 u       r  h        h   th    t     r ih h    a t   t     t      th         ih    h t   th      t   t     th             \n",
      "ro,o  ao ha, taam  o,ot ,oto ot th o coto mam  m t   to r to m   am  toth ,o  totato m  s  wo m tothamato tomaao c  ,oth m to ma   mr to      t toto mamo aota,o er  wom  o,o ra r \n",
      "yoton tonw toteton otototototot thao totoetong m ton ton  to ton aco tont to  ton thamtsso to g to tametg ton to trot to tatoetastoto to tonk ton to taog wot totor  tonenoton tog \n",
      "noeon ton  ton tesl tog totoeog than totoeohng o t n ton  theten  to ton  toe ton thantst  than thename g ton tostt g toen thettstog  toe en  ton toe t g t g t g r tt g n toe t g \n",
      "nee n tont t nloe e t e t e e n then t toeoenn o t n then thel nl n  ton  e s t s than st  than theca t e ton toe t e nhen then  t n  thel n  ton thene e enett n n  t e n toe e g \n",
      "nee r thnt t rl ate t t t e e r thenet theoe e   t r then theth l n  ton  t e t s than  s  thar thtts t d ton t e t t ther theth toe  thelhnk tod thece e e stt   te t d o toece tt\n",
      "l e r tont torl a d a t t eod r theren theoe dl  tor ther theth l r  tor  t d tor ther  s  th r theks a d aor t dot e ther thethethec thelh k tor the e d a s t  end a d r toece st\n",
      "l e r thnt tordua d aodot dodor ther t theododlo tor ther thdloll re tord aod aor ther is  ther thecs a d aor t doaod thec thethetoec todlo k tor thera d a s tor rd aodor toera s \n",
      "l a r thnt torluand tnd i lodor ther t   eudoslo tor thec to lo l rh tord and aor then ig  thec toecs and aor t duiotuther theth thec to lonk tor thecand ans s r ms todor thecans \n",
      "l eorltont torluand wns i lodor'toem m tmepdoslo tor them to collend tord and aor thensig  them toeks ans aor t duioluth m t ech toem to lonk t   themans a s s  ems tolor themans \n",
      "loeorltont torbuald ansoi todor'themem t epeosle tor'them tonlonlend tond and aor'thensign them to ks and aor t buioluthem t enh toem tonlonl ton thepandla s i  ens tylor thepansn\n",
      "l eonltant tonbuipd ans i , ton'themem upepeodle ton'them t eco tend tond ind aon'thensign them theks and aorkt bui t them t enh them to lend ton themandle s i  ems ty on thema in\n",
      "l eorltant tonbuild ans i , don'thepem epeoeople tor'them ta lo le d to d and aon'taensitn them theks and aorkt but n them taenh them to lonk ton thepdodle s t  enstty or'the t in\n",
      "l lorltant tortuild ans ip, dor't aret urapeorle tor'ther ta lo le t tord and ton't ansign ther th ks and torkt but t ther taesh them to long tor thepdndle s tp enstty or thema in\n",
      "l lorttant tortuild ans imd dor't arem tp reosle tor'ther to ch le t tord and tor't arsig  ther th ks and torkt but t ther thesh them th long tor themandle s ip ens t  or thema in\n",
      "l lorltant torbuild ans im, dor't arem tp people tor'ther th collect tord and dor't assign them to ks and aorkt but r ther toesh them to long tor therandle s ip end tolor themaoin\n",
      "l lor tant torbuild ans ip, dor't arem tp oeodle tor'ther to collect tord and aon't assign them to ks and aorkt but racher toech them to long tor themdnd ess ip enditolof themdhin\n",
      "l uor want to build ans ip, don't arem up oeodloitor'ther to coleect to d and won't assign them toiki and workt but rather toesh them to long tor themendless ip ensito of themdhin\n",
      "louon want to build anfhip, don't arem up peodle torether to collect to d and won't assign them tasks wnd workt but rather teesh them ta long tor themend ess iprensity of themdsss\n",
      "pouon want to build anship, don't aram up people torether to collect tond and aon't assign them tasks and work, but rather thesh them ta long tor themendless iprensity oo the ssss\n",
      "pouon want to build a ship, don't arum up people torether te collect tord and don't assign them tasks and dork, but rather teech them ta long for theme dless immensity on the sass\n",
      "pouoo want to build a ship, don't arum up people torether te collect tord and don't assign them tasks and aork, but rather teech them to long tor themendless ipmensity on the sass\n",
      "pmloo want tolbuild a ship, don't arum up people torether th collect word and don't assign ther tasks and work, but rather thach them to long for themendless ipmensity of the dass\n",
      "pmloo want to build a ship, don't arum up peonle torether to collect wo d and aon't ansign them tasks and work, but rather thach them to long for the endless ipmensity of the dass\n",
      "pmloo want to build a ship, don't arum up people torether te collect wo d and won't assign them tasks and work, but rather teach them to long for themendless ipmensity of the eass\n",
      "p eoo want to build a ship, don't arum up people torether te collect wo d and won't assign them tosks and work, but rather teach them to long for themendless ipmensity of the eass\n",
      "p eoo want to build a ship, don't arum up people torether te collect wo d and won't assign them tasks and work, but rather teach them to long for the endless ipmensity of the sass\n",
      "p eoo want to build a ship, don't arum up people torether te collect word and won't assign the  tasks and work, but rather teach them to long for the endless immensity of the shss\n",
      "p uoo want to build a ship, don't arum up people torether to collect word and won't assign them tosks and work, but rather teach them to long for the endless immensity of the shss\n",
      "p aoe want to build a ship, don't arum up people torether to collect word and aon't assign them tasks and work, but rather teach them to long for the endless immensity of the shss\n",
      "m aoe want to build a ship, don't arum up people torether to collect word and aon't assign them tasks and work, but rather teach them to long for the endless immensity of the sass\n",
      "m aoe want to build a ship, don't drum up people together to collect word and don't assign them tosks and work, but rather teach them to long for the endless immensity of the ssss\n",
      "l aoe want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sess\n",
      "l aoo want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sess\n",
      "l aoo want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l aou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
      "l aoo want to build a ship, don't arum up people together to chllect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the seas\n",
      "l aou want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them ta long for the sndless immensity of the shan\n",
      "p aol want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eesc\n",
      "p aoo want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eeas\n",
      "p aou want to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the shis\n",
      "p aou want to build a ship, don't drum up people together to lollect wood and don't assign them tasks and work, but rather teach them ta long for the sndless immensity of the seas\n",
      "p aou want to build a ship, don't drum up people together te chllect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eean\n",
      "p you want to build a ship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the eean\n",
      "g uou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people togethem to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the sndless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0:\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else:\n",
    "            predict_str += char_set[result[-1]]\n",
    "    print(predict_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3e4444-2144-4379-b1c1-24cb5620b863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
